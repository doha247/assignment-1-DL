# -*- coding: utf-8 -*-
"""assignment 1 DL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a2Rd_N63kMFNXFIpG6rlMoUMhDy_201f
"""

!pip -q install tensorflow-datasets
import tensorflow as tf
import tensorflow_datasets as tfds

IMG_SIZE = (224, 224)
BATCH_SIZE = 32

(ds_train, ds_val), info = tfds.load(
    "oxford_flowers102",
    split=["train", "validation"],
    as_supervised=True,
    with_info=True
)

num_classes = info.features["label"].num_classes
print("Num classes:", num_classes)

from tensorflow.keras import layers

data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.15),
    layers.RandomZoom(0.15),
    layers.RandomContrast(0.1),
])

def preprocess(img, label):
    img = tf.image.resize(img, IMG_SIZE)
    img = tf.cast(img, tf.float32)
    return img, label

AUTOTUNE = tf.data.AUTOTUNE

train_ds = (ds_train
            .map(preprocess, num_parallel_calls=AUTOTUNE)
            .shuffle(2000)
            .batch(BATCH_SIZE)
            .prefetch(AUTOTUNE))

val_ds = (ds_val
          .map(preprocess, num_parallel_calls=AUTOTUNE)
          .batch(BATCH_SIZE)
          .prefetch(AUTOTUNE))

from tensorflow.keras import models
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.applications.densenet import preprocess_input

base_model = DenseNet121(
    weights="imagenet",
    include_top=False,
    input_shape=(224, 224, 3)
)
base_model.trainable = False

inputs = layers.Input(shape=(224, 224, 3))
x = data_augmentation(inputs)
x = layers.Lambda(preprocess_input)(x)
x = base_model(x, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.4)(x)
x = layers.Dense(512, activation="relu")(x)
outputs = layers.Dense(num_classes, activation="softmax")(x)

model = models.Model(inputs, outputs)

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

model.summary()

callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.2)
]

history1 = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,
    callbacks=callbacks
)

base_model.trainable = True

for layer in base_model.layers[:-50]:
    layer.trainable = False

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-5),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

history2 = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,
    callbacks=callbacks
)

loss, acc = model.evaluate(val_ds)
print("Validation accuracy:", acc)

model.save("oxford_flowers102_densenet121_finetuned.h5")
print("Saved: oxford_flowers102_densenet121_finetuned.h5")

import matplotlib.pyplot as plt

acc = history1.history["accuracy"] + history2.history["accuracy"]
val_acc = history1.history["val_accuracy"] + history2.history["val_accuracy"]

plt.figure()
plt.plot(acc)
plt.plot(val_acc)
plt.legend(["Train Acc", "Val Acc"])
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.show()

